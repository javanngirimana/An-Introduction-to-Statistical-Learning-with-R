---
title: "Chap4_Classification"
author: "Javan"
date: "2026-02-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

# CHAP 4 Classification
# 4.7 Lab: Classification Methods
# 4.7.1 The Stock Market Data

```{r}
library(ISLR2)
names(Smarket)
```

```{r}
 dim(Smarket)
```
```{r}
summary(Smarket)
```

```{r}
pairs(Smarket)
```

# The cor() function produces a matrix that contains all of the pairwise correlations among the predictors in a data set.

Error in cor(Smarket) : `x' must be numeric
```{r}
cor(Smarket)
```

```{r}
cor(Smarket[, -9])
```

# By plotting the data, which is ordered chronologically, we see that Volume is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005.

```{r}
attach(Smarket)
plot(Volume)
```
# 4.7.2 Logistic Regression
The glm() function can be used to fit many types of generalized linear models, including logistic regression. 
```{r}
glm.fits <- glm(
Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = Smarket, family = binomial
)
summary(glm.fits)
```

# We use the coef() function in order to access just the coefficients for this fitted model. 
We can also use the summary() function to access particular aspects of the fitted model, such as the p-values for the coefficients.
```{r}
coef(glm.fits)
```
```{r}
summary(glm.fits)$coef
```
```{r}
summary(glm.fits)$coef[, 4]
```

# The predict() function can be used to predict the probability that the market will go up, given values of the predictors.
```{r}
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]
```

# We know that these values correspond to the probability of the market going up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.
```{r}
contrasts(Direction)
```

#In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilitiesm into class labels, Up or Down.
```{r}
glm.pred <- rep("Down", 1250)
glm.pred[glm.probs > .5] = "Up"
```

# Given these predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.
```{r}
table(glm.pred, Direction)
```

```{r}
mean(glm.pred == Direction)
```
# To implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004.
We will then use this vector to create a held out data set of observations from 2005.

```{r}
train <- (Year < 2005)
Smarket.2005 <- Smarket[!train, ]
dim(Smarket.2005)
```

```{r}
Direction.2005 <- Direction[!train]
```

The object train is a vector of 1,250 elements, corresponding to the observations in our data set.

# We now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument.
We then obtain predicted probabilities of the stock market going up for each of the days in our test set—that is, for the days in 2005.
```{r}
glm.fits <- glm(
Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = Smarket, family = binomial, subset = train
)
glm.probs <- predict(glm.fits, Smarket.2005,
type = "response")
```

#  Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period.
```{r}
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
```

```{r}
mean(glm.pred == Direction.2005)
```

```{r}
mean(glm.pred != Direction.2005)
```

# Below we have refit the logistic regression using just Lag1 and Lag2, which seemed to have the highest predictive power in the original logistic regression model.
```{r}
glm.fits <- glm(Direction ~ Lag1 + Lag2, data = Smarket,
family = binomial, subset = train)
glm.probs <- predict(glm.fits, Smarket.2005,
type = "response")
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
```

```{r}
mean(glm.pred == Direction.2005)
```

```{r}
 106 / (106 + 76)
```

#  In particular, we want to predict Direction on a day when Lag1 and Lag2 equal 1.2 and 1.1, respectively, and on a day when they equal 1.5 and −0.8. We do this using the predict() function.
```{r}
predict(glm.fits,
newdata =
data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),
type = "response"
)
```

# 4.7.3 Linear Discriminant Analysis
In R, we fit an LDA model using the lda() function, which is part of the MASS library.
```{r}
library(MASS)
lda.fit <- lda(Direction ~ Lag1 + Lag2, data = Smarket,
subset = train)
lda.fit

```
```{r}
plot(lda.fit)
```

The second element, posterior, is a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class, computed from (4.15). Finally, x contains the linear discriminants, described earlier.
```{r}
lda.pred <- predict(lda.fit, Smarket.2005)
names(lda.pred)
```

# As we observed in Section 4.5, the LDA and logistic regression predictions are almost identical.
```{r}
lda.class <- lda.pred$class
table(lda.class, Direction.2005)
```
```{r}
mean(lda.class == Direction.2005)
```

# Applying a 50 % threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class.
```{r}
sum(lda.pred$posterior[, 1] >= .5)
sum(lda.pred$posterior[, 1] < .5)
```

# Notice that the posterior probability output by the model corresponds to the probability that the market will decrease:
```{r}
lda.pred$posterior[1:20, 1]
lda.class[1:20]
```

For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day—say, if the posterior probability is at least 90 %.
```{r}
sum(lda.pred$posterior[, 1] > .9)
```

# 4.7.4 Quadratic Discriminant Analysis
# We will now fit a QDA model to the Smarket data. QDA is implemented in R using the qda() function, which is also part of the MASS library.
```{r}
qda.fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket,
subset = train)
qda.fit

```
# The predict() function works in exactly the same fashion as for LDA.
```{r}
qda.class <- predict(qda.fit, Smarket.2005)$class
table(qda.class, Direction.2005)
```

```{r}
mean(qda.class == Direction.2005)
```

# 4.7.5 Naive Bayes
we fit a naive Bayes model to the Smarket data. Naive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library.
```{r}
library(e1071)
nb.fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket,
subset = train)
nb.fit
```

The output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for Lag1 is 0.0428 for Direction=Down, and the standard deviation is 1.23. We can easily verify this:
```{r}
mean(Lag1[train][Direction[train] == "Down"])
sd(Lag1[train][Direction[train] == "Down"])
```

# The predict() function is straightforward.
```{r}
nb.class <- predict(nb.fit, Smarket.2005)
table(nb.class, Direction.2005)
```

```{r}
mean(nb.class == Direction.2005)
```

# The predict() function can also generate estimates of the probability that each observation belongs to a particular class.
```{r}
nb.preds <- predict(nb.fit, Smarket.2005, type = "raw")
nb.preds[1:5, ]
```


# 4.7.6 K-Nearest Neighbors
We use the cbind() function, short for column bind, to bind the Lag1 and Lag2 variables together into two matrices, one for the training set and the other for the test set.
```{r}
library(class)
train.X <- cbind(Lag1, Lag2)[train, ]
test.X <- cbind(Lag1, Lag2)[!train, ]
train.Direction <- Direction[train]
```

# Now the knn() function can be used to predict the market’s movement for the dates in 2005.
```{r}
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.2005)
```

# Below, we repeat the analysis using K = 3.
```{r}
knn.pred <- knn(train.X, test.X, train.Direction, k = 3)
table(knn.pred, Direction.2005)
```

```{r}
mean(knn.pred == Direction.2005)
```


# In this data set, only 6 % of people purchased caravan insurance.
```{r}
dim(Caravan)
```
```{r}
attach(Caravan)
summary(Purchase)
```

# The scale() function does just this. In standardizing the data, we exclude column 86, because that is the qualitative Purchase variable.
```{r}
standardized.X <- scale(Caravan[, -86])
var(Caravan[, 1])
```
```{r}
var(Caravan[, 2])
```
```{r}
var(standardized.X[, 1])
```
```{r}
var(standardized.X[, 2])
```

# We fit a KNN model on the training data using K = 1, and evaluate its performance on the test data.

```{r}
test <- 1:1000
train.X <- standardized.X[-test, ]
test.X <- standardized.X[test, ]
train.Y <- Purchase[-test]
test.Y <- Purchase[test]
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Y, k = 1)
mean(test.Y != knn.pred)
```

```{r}
mean(test.Y != "No")
```

# Among 77 such customers, 9, or 11.7 %, actually do purchase insurance. This is double the rate that one would obtain from random guessing.
```{r}
table(knn.pred, test.Y)
```

Using K = 3, the success rate increases to 19 %, and with K = 5 the rate is 26.7 %. This is over four times the rate that results from random guessing.
It appears that KNN is finding some real patterns in a difficult data set!
```{r}
knn.pred <- knn(train.X, test.X, train.Y, k = 3)
table(knn.pred, test.Y)
```

```{r}
knn.pred <- knn(train.X, test.X, train.Y, k = 5)
table(knn.pred, test.Y)
```

```{r}
 4 / 15
```
If we instead predict a purchase any time the predicted probability of purchase exceeds 0.25, we get much better results: we predict that 33 people will purchase insurance, and we are correct for about 33 % of these people. This is over five times better
than random guessing!
```{r}
glm.fits <- glm(Purchase ~ ., data = Caravan,
family = binomial, subset = -test)
```

```{r}
glm.probs <- predict(glm.fits, Caravan[test, ],
type = "response")
glm.pred <- rep("No", 1000)
glm.pred[glm.probs > .5] <- "Yes"
table(glm.pred, test.Y)
```

```{r}
glm.pred <- rep("No", 1000)
glm.pred[glm.probs > .25] <- "Yes"
table(glm.pred, test.Y)
```

```{r}
11 / (22 + 11)
```
Finally, we fit a Poisson regression model to the Bikeshare data set, which measures the number of bike rentals (bikers) per hour in Washington, DC.
The data can be found in the ISLR2 library.
```{r}
attach(Bikeshare)
dim(Bikeshare)
```
```{r}
names(Bikeshare)
```
# We begin by fitting a least squares linear regression model to the data.
```{r}
mod.lm <- lm(
bikers ~ mnth + hr + workingday + temp + weathersit,
data = Bikeshare
)
summary(mod.lm)
```
# The results seen in Section 4.6.1 used a slightly different coding of the variables hr and mnth, as follows:
```{r}
contrasts(Bikeshare$hr) = contr.sum(24)
contrasts(Bikeshare$mnth) = contr.sum(12)
mod.lm2 <- lm(
bikers ~ mnth + hr + workingday + temp + weathersit,
data = Bikeshare
)
summary(mod.lm2)
```
# For example, we see that the predictions from the linear model are the same regardless of coding:
```{r}
sum((predict(mod.lm) - predict(mod.lm2))^2)
```

# The sum of squared differences is zero. We can also see this using the all.equal() function:
```{r}
all.equal(predict(mod.lm), predict(mod.lm2))
```
# The coefficient for December must be explicitly computed as the negative sum of all the other months.
```{r}
coef.months <- c(coef(mod.lm2)[2:12],
-sum(coef(mod.lm2)[2:12]))
```

# To make the plot, we manually label the x-axis with the names of the months.
```{r}
plot(coef.months, xlab = "Month", ylab = "Coefficient",
xaxt = "n", col = "blue", pch = 19, type = "o")
axis(side = 1, at = 1:12, labels = c("J", "F", "M", "A",
"M", "J", "J", "A", "S", "O", "N", "D"))
```
# Reproducing the right-hand side of Figure 4.13 follows a similar process.
```{r}
coef.hours <- c(coef(mod.lm2)[13:35],
-sum(coef(mod.lm2)[13:35]))
plot(coef.hours, xlab = "Hour", ylab = "Coefficient",
col = "blue", pch = 19, type = "o")
```

Now, we consider instead fitting a Poisson regression model to the Bikeshare data. Very little changes, except that we now use the function glm() with the argument family = poisson to specify that we wish to fit a Poisson regression model:

```{r}
mod.pois <- glm(
bikers ~ mnth + hr + workingday + temp + weathersit,
data = Bikeshare, family = poisson
)
summary(mod.pois)
```

# We can plot the coefficients associated with mnth and hr, in order to reproduce Figure 4.15:
```{r}
coef.mnth <- c(coef(mod.pois)[2:12],
-sum(coef(mod.pois)[2:12]))
plot(coef.mnth, xlab = "Month", ylab = "Coefficient",
xaxt = "n", col = "blue", pch = 19, type = "o")
axis(side = 1, at = 1:12, labels = c("J", "F", "M", "A", "M", "J"
, "J", "A", "S", "O", "N", "D"))
coef.hours <- c(coef(mod.pois)[13:35],
-sum(coef(mod.pois)[13:35]))
plot(coef.hours, xlab = "Hour", ylab = "Coefficient",
col = "blue", pch = 19, type = "o")
```

However, we must use the argument type = "response" to specify that we want R to output exp(βˆ0 + βˆ1X1+...+βˆpXp) rather than βˆ0+βˆ1X1+...+βˆpXp, which it will output by default.
```{r}
plot(predict(mod.lm2), predict(mod.pois, type = "response"))
abline(0, 1, col = 2, lwd = 3)
```






































































