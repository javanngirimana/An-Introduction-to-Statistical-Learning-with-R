---
title: "Chap8_Decision Trees"
author: "Javan"
date: "2026-02-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.


# 8.3 Lab: Decision Trees
# 8.3.1 Fitting Classification Trees

```{r}
# install.packages("tree")
```

```{r}
library(tree)
```

```{r}
library(ISLR2)
attach(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
```


# Finally, we use the data.frame() function to merge High with the rest of the Carseats data.

```{r}
Carseats <- data.frame(Carseats, High)
```

# The syntax of the tree() function is quite similar to that of the lm() function.
```{r}
tree.carseats <- tree(High ~ . - Sales, Carseats)
```


```{r}
summary(tree.carseats)
```


```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 1, cex = 0.7)
```


# Branches that lead to terminal nodes are indicated using asterisks.
```{r}
tree.carseats
```
# 8.3.2 Fitting Regression Trees

# Here we fit a regression tree to the Boston data set. First, we create a training set, and fit the tree to the training data.

```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
```
# In the context of a regression tree, the deviance is simply the sum of squared errors for the tree. We now plot the tree.

```{r}
plot(tree.boston)
text(tree.boston, pretty = 0, cex=0.6)
```

# Now we use the cv.tree() function to see whether pruning the tree will improve performance.
```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```
# However, if we wish to prune the tree, we could do so as follows, using the prune.tree() function:
```{r}
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0, cex=0.7)
```
# In keeping with the cross-validation results, we use the unpruned tree to make predictions on the test set.

```{r}
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
```
# 8.3.3 Bagging and Random Forests
# Therefore, the randomForest() function can be used to perform both random forests and bagging. We perform bagging as follows:

```{r}
# install.packages("randomForest")
```

```{r}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston,
    subset = train, mtry = 12, importance = TRUE)
bag.boston
```

# The argument mtry = 12 indicates that all 12 predictors should be considered for each split of the tree—in other words, that bagging should be done.
How well does this bagged model perform on the test set?

```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
```
# We could change the number of trees grown by randomForest() using the ntree argument:
```{r}
bag.boston <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
```
By default, randomForest() uses p/3 variables when building a random forest of regression trees, and √p variables when building a random forest of classification trees. Here we use mtry = 6.

```{r}
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
```

# Using the importance() function, we can view the importance of each variable.
```{r}
importance(rf.boston)
```
# Plots of these importance measures can be produced using the varImpPlot() function.
```{r}
varImpPlot(rf.boston)
```
# 8.3.4 Boosting
The argument n.trees = 5000 indicates that we want 5000 trees, and the option interaction.depth = 4 limits the depth of each tree.

```{r}
# install.packages("gbm")
```


```{r}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4)
```

# The summary() function produces a relative influence plot and also outputs the relative influence statistics.
```{r}
summary(boost.boston)
```
#  In this case, as we might expect, median house prices are increasing with rm and decreasing with lstat.
```{r}
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
```

# We now use the boosted model to predict medv on the test set:
```{r}
yhat.boost <- predict(boost.boston,
newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```
# If we want to, we can perform boosting with a different value of the shrinkage parameter λ in (8.10). The default value is 0.001, but this is easily modified. Here we take λ = 0.2.
```{r}
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston,
newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```



# In the case of a classification tree, the argument type = "class" instructs R to return the actual class prediction. This approach leads to correct predictions for around 77 % of the locations in the test data set.
```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
tree.carseats <- tree(High ~ . - Sales, Carseats,
subset = train)
tree.pred <- predict(tree.carseats, Carseats.test,
type = "class")
table(tree.pred, High.test)
```


# 8.3.5 Bayesian Additive Regression Trees
# The gbart() function is designed for quantitative outcome variables. For binary outcomes, lbart() and pbart() are available.

To run the gbart() function, we must first create matrices of predictors for the training and test data. We run BART with default settings.

```{r}
# install.packages("BART")
```

```{r}
library(BART)
x <- Boston[, 1:12]
y <- Boston[, "medv"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
```


# Next we compute the test error.
```{r}
yhat.bart <- bartfit$yhat.test.mean
mean((ytest - yhat.bart)^2)
```

On this data set, the test error of BART is lower than the test error of random forests and boosting.
Now we can check how many times each variable appeared in the collection of trees.
```{r}
ord <- order(bartfit$varcount.mean, decreasing = T)
bartfit$varcount.mean[ord]
```
```{r}
# install.packages("rpart.plot")
```



```{r}
library(rpart)
library(rpart.plot)
tree.fit <- rpart(y ~ ., data = train)
rpart.plot(tree.fit)

```





