---
title: "Chap6_Linear Model Selection and Regularization"
author: "Javan"
date: "2026-02-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

# Chap6_Linear Model Selection and Regularization
# 6.5.1 Subset Selection Methods
# The sum() function can then be used to count all of the missing elements.

```{r}
library(ISLR2)
names(Hitters)
```

```{r}
dim(Hitters)
```
```{r}
sum(is.na(Hitters$Salary))
```
# Hence we see that Salary is missing for 59 players. The na.omit() function removes all of the rows that have missing values in any variable.
```{r}
Hitters <- na.omit(Hitters)
dim(Hitters)
```

```{r}
sum(is.na(Hitters))
```

# The summary() command outputs the best set of variables for each model size.
```{r}
# install.packages("leaps")
library(leaps)
regfit.full <- regsubsets(Salary ~ ., Hitters)
summary(regfit.full)
```

# But the nvmax option can be used in order to return as many variables as are desired. Here we fit up to a 19-variable model.
```{r}
regfit.full <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19)
reg.summary <- summary(regfit.full)
```

# The summary() function also returns R2, RSS, adjusted R2, Cp, and BIC. We can examine these to try to select the best overall model.
```{r}
names(reg.summary)
```

# As expected, the R2 statistic increases monotonically as more variables are included.
```{r}
reg.summary$rsq
```

```{r}
par(mfrow = c(2, 2))
plot(reg.summary$rss, xlab = "Number of Variables",
ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables",
ylab = "Adjusted RSq", type = "l")
```

# The which.max() function can be used to identify the location ofthe maximum point of a vector. We will now plot a red dot to indicate the model with the largest adjusted R2 statistic.
```{r}
which.max(reg.summary$adjr2)
plot(reg.summary$adjr2, xlab = "Number of Variables",
ylab = "Adjusted RSq", type = "l")
points(11, reg.summary$adjr2[11], col = "red", cex = 2,
pch = 20)
```

# In a similar fashion we can plot the Cp and BIC statistics, and indicate the models with the smallest statistic using which.min()
```{r}
plot(reg.summary$cp, xlab = "Number of Variables",
ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col = "red", cex = 2,
pch = 20)
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables",
ylab = "BIC", type = "l")
points(6, reg.summary$bic[6], col = "red", cex = 2,
pch = 20)
```

The regsubsets() function has a built-in plot() command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, Cp, adjusted R2, or AIC. To find out more about this function, type ?plot.regsubsets.

```{r}
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
```
#We can use the coef() function to see the coefficient estimates associated with this model.
```{r}
coef(regfit.full, 6)
```
# Forward and Backward Stepwise Selection
```{r}
 regfit.fwd <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19, method = "backward")
summary(regfit.bwd)
```

# However, the best seven-variable models identified by forward stepwise selection, backward stepwise selection, and best subset selection are different.
```{r}
coef(regfit.full, 7)
```

```{r}
coef(regfit.fwd, 7)
```


```{r}
coef(regfit.bwd, 7)
```


# Choosing Among Models Using the Validation-Set Approach and Cross-Validation
We also set a random seed so that the user will obtain the same training set/test set split.
```{r}
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters),
replace = TRUE)
test <- (!train)
```

Now, we apply regsubsets() to the training set in order to perform best subset selection.

```{r}
regfit.best <- regsubsets(Salary ~ .,
data = Hitters[train, ], nvmax = 19)
```

We first make a model matrix from the test data.

```{r}
test.mat <- model.matrix(Salary ~ ., data = Hitters[test, ])
```


Now we run a loop, and for each size i, we extract the coefficients from regfit.best for the best model of that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.

```{r}
val.errors <- rep(NA, 19)
for (i in 1:19) {
coefi <- coef(regfit.best, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
```

# We find that the best model is the one that contains seven variables.

```{r}
val.errors
```

```{r}
which.min(val.errors)
```

```{r}
coef(regfit.best, 7)
```

# Since we will be using this function again, we can capture our steps above and write our own predict method.

```{r}
predict.regsubsets <- function(object, newdata , id, ...) {
 form <- as.formula(object$call[[2]])
 mat <- model.matrix(form, newdata)
 coefi <- coef(object, id = id)
 xvars <- names(coefi)
 mat[, xvars] %*% coefi
 }
```



Note that we perform best subset selection on the full data set and select the best seven-variable model, rather than simply using the variables that were obtained from the training set, because the best seven-variable model on the full data set may differ from the corresponding model on the training set.

```{r}
regfit.best <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19)
coef(regfit.best, 7)
```

First, we create a vector that allocates each observation to one of k = 10 folds, and
we create a matrix in which we will store the results.
```{r}
k <- 10
n <- nrow(Hitters)
set.seed(1)
folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 19,
dimnames = list(NULL, paste(1:19)))
```

Note that in the following code R will automatically use our predict.regsubsets() function
when we call predict() because the best.fit object has class regsubsets.

```{r}
for (j in 1:k) {
best.fit <- regsubsets(Salary ~ .,
data = Hitters[folds != j, ],
nvmax = 19)
for (i in 1:19) {
pred <- predict(best.fit, Hitters[folds == j, ], id = i)
cv.errors[j, i] <-
mean((Hitters$Salary[folds == j] - pred)^2)
}
}
```

We use the apply() function to average over the columns of this matrix in order to obtain a vector for which the ith element is the crossvalidation error for the i-variable model.
```{r}
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
```

```{r}
par(mfrow = c(1, 1))
plot(mean.cv.errors, type = "b")
```


# We see that cross-validation selects a 10-variable model. We now perform best subset selection on the full data set in order to obtain the 10-variable model.
```{r}
reg.best <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19)
coef(reg.best, 10)
```

# 6.5.2 Ridge Regression and the Lasso
```{r}
x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary
```


# Ridge Regression
# Install the package
```{r}
# install.packages("glmnet")

```

```{r}
library(glmnet)

```



```{r}
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)

```

```{r}
dim(coef(ridge.mod))

```


```{r}
ridge.mod$lambda[50]
coef(ridge.mod)[, 50]
```

```{r}
sqrt(sum(coef(ridge.mod)[-1, 50]^2))
```

```{r}
ridge.mod$lambda[60]
coef(ridge.mod)[, 60]
```


```{r}
sqrt(sum(coef(ridge.mod)[-1, 60]^2))
```

# We can use the predict() function for a number of purposes.
```{r}
predict(ridge.mod, s = 50, type = "coefficients")[1:20, ]
```

# Here we demonstrate the latter approach. We first set a random seed so that the results obtained will be reproducible.

```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
```

# Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.

```{r}
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0,
lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test, ])
mean((ridge.pred - y.test)^2)
```

The test MSE is 142,199. Note that if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this:

```{r}
mean((mean(y[train]) - y.test)^2)
```

```{r}
ridge.pred <- predict(ridge.mod, s = 0, newx = x[test, ],
exact = T, x = x[train, ], y = y[train])
mean((ridge.pred - y.test)^2)

lm(y ~ x, subset = train)
predict(ridge.mod, s = 0, exact = T, type = "coefficients",
x = x[train, ], y = y[train])[1:20, ]
```


#  Note that we set a random seed first so our results will be reproducible, since the choice of the cross-validation folds is random.

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```



# Therefore, we see that the value of 位 that results in the smallest crossvalidation error is 326. What is the test MSE associated with this value of 位?
```{r}
ridge.pred <- predict(ridge.mod, s = bestlam,
newx = x[test, ])
mean((ridge.pred - y.test)^2)
```
# Finally, we refit our ridge regression model on the full data set, using the value of 位 chosen by cross-validation, and examine the coefficient estimates.
```{r}
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:20, ]
```

# In order to fit a lasso model, we once again use the glmnet() function; however, this time we use the argument alpha=1. Other than that change, we proceed just as we did in fitting a ridge model.

```{r}
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1,
lambda = grid)
plot(lasso.mod)
```


#  We now perform cross-validation and compute the associated test error.
```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam,
newx = x[test, ])
mean((lasso.pred - y.test)^2)
```

# So the lasso model with 位 chosen by cross-validation contains only eleven variables.
```{r}
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients",
s = bestlam)[1:20, ]
```


# 6.5.3 PCR and PLS Regression
# Principal Components Regression

```{r}
# install.packages("pls")
```


```{r}
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary ~ ., data = Hitters, scale = TRUE,
validation = "CV")
```

Setting validation = "CV" causes pcr() to compute the ten-fold cross-validation error for each possible value of M, the number of principal components used. The resulting fit can be examined using summary().
```{r}
summary(pcr.fit)
```


# One can also plot the cross-validation scores using the validationplot() function. Using val.type = "MSEP" will cause the cross-validation MSE to be plotted.
```{r}
validationplot(pcr.fit, val.type = "MSEP")
```


In contrast, using M = 5 increases the value to 84.29 %. If we were to use all M = p = 19 components, this would increase to 100 %. We now perform PCR on the training data and evaluate its test set performance.

```{r}
set.seed(1)
pcr.fit <- pcr(Salary ~ ., data = Hitters, subset = train,
scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
```


# Now we find that the lowest cross-validation error occurs when M = 5 components are used. We compute the test MSE as follows.
```{r}
pcr.pred <- predict(pcr.fit, x[test, ], ncomp = 5)
mean((pcr.pred - y.test)^2)
```

# Finally, we fit PCR on the full data set, using M = 5, the number of components identified by cross-validation.

```{r}
pcr.fit <- pcr(y ~ x, scale = TRUE, ncomp = 5)
summary(pcr.fit)
```

# Partial Least Squares
# We implement partial least squares (PLS) using the plsr() function, also plsr() in the pls library. The syntax is just like that of the pcr() function.
```{r}
set.seed(1)
pls.fit <- plsr(Salary ~ ., data = Hitters, subset = train, scale
= TRUE, validation = "CV")
summary(pls.fit)
```
```{r}
validationplot(pls.fit, val.type = "MSEP")
```
# The lowest cross-validation error occurs when only M = 1 partial least squares directions are used. We now evaluate the corresponding test set MSE.
```{r}
pls.pred <- predict(pls.fit, x[test, ], ncomp = 1)
mean((pls.pred - y.test)^2)
```


# Finally, we perform PLS using the full data set, using M = 1, the number of components identified by cross-validation.
```{r}
pls.fit <- plsr(Salary ~., data = Hitters, scale = TRUE,
ncomp = 1)
summary(pls.fit)
```































